<!DOCTYPE html>
<html lang="en-US" dir="ltr">

    <head>
        <!-- Header -->
        <script language="javascript" type="text/javascript" src="https://www.bbgatch.com/header.js"></script>
        <title>bbgatch | Forecasting TSA Data with R</title>
    </head>

    <body>
        <h1>Forecasting TSA Data with R</h1>
            <p><i>2024-09-27</i></p>
            
        <!-- TODO:
            Show the results of just ARIMA and ETS first and then show how adding combined improves results.
            Check alt text

        -->

            <p>If we want to create a forecast in R for the TSA Passenger data to predict passenger volumes in the future, we can use the <a href="https://tidyverts.org">tidyverts packages</a> `fable`, `tsibble`, and `feasts`.</p>

            <p>These packages apply <a href="https://r4ds.had.co.nz/tidy-data.html">tidy data principles</a> to time series data and forecasting. They were coauthored by the forecasting great <a href="https://robjhyndman.com">Rob Hyndman</a>. If you are interested in learning more about forecasting, I highly recommend Rob Hyndman and George Athanasopoulos's fantastic textbook: <a href="https://otexts.com/fpp3/">Forecasting: Principles and Practice (3rd Edition) (FPP3)</a>, which uses these tidyverts packages and workflows.</p>
            
            <h2>Plotting the History</h2>
                <p>The historical TSA Passenger data that we have looks like this:</p>
                <img src="images/tsa-passenger-history.png" alt="Line plot of TSA Passenger History." class="responsive" width="80%">

                <p><a href="https://www.bbgatch.com/projects/tsa/2024-08-27-how-much-history-to-use-forecasting-tsa-data/2024-08-27-how-much-history-to-use-forecasting-tsa-data.html">We recently learned</a> that our models will likely perform best if we ignore pre-Covid data and just assume history begins at April 2020. So we will use that in our model selection and final forecasts here.</p>
                
                <p>There is clear seasonality in the data, and there has been an upward trend coming out of the Covid downturn.</p>
                <img src="images/time-series-decomposition.png" alt="Line plots decomposing TSA Passenger History time series into trend, seasonality, and remainder components." class="responsive" width="80%">
                
            <h2>Evaluating Models with a Train/Test Split</h2>
                <p>The <code>fable</code> package provides several time series models to choose from. We can evaluate which models perform the best by breaking our data into separate train/test sets. We then fit each model on the training data, generate a forecast from each model over the test set time period, and we evaluate each model's forecast against our test set to measure accuracy.</p>
                
                <p>We'll start with <a href="https://otexts.com/fpp3/simple-methods.html">four simple baseline models</a> that allow us to compare our more complex models against simple but still effective baselines. We'll use the <code>ARIMA</code> function to find the best fitting ARIMA model, and we'll try four different ETS models. We know that trend and seasonality is likely to continue, so we will limit ourselves to just these ETS models, but you could try others. Alternatively, the <code>ETS()</code> function can select the best-fitting ETS model for you, just like the <code>ARIMA()</code> function.</p>
                
                <pre><code class="language-r">
                    fit <- train |> model(
                        # Baseline models
                        mean = MEAN(passengers),
                        naive = NAIVE(passengers),
                        drift = NAIVE(passengers ~ drift()),
                        snaive = SNAIVE(passengers),
                        # tslm = TSLM(passengers ~ trend() + season()),
                        
                        # Auto-ARIMA and Auto-ETS models
                        arima = ARIMA(passengers, stepwise=FALSE, approximation=FALSE),
                        # ets = ETS(passengers),
                
                        # Specify ETS models
                        ets_aaa = ETS(passengers ~ error('A') + trend('A') + season('A')),
                        ets_aada = ETS(passengers ~ error('A') + trend('Ad') + season('A')),
                        ets_aam = ETS(passengers ~ error('A') + trend('A') + season('M')),
                        ets_aadm = ETS(passengers ~ error('A') + trend('Ad') + season('M'))                
                </code></pre>

                <img src="images/train-test-model-accuracy-initial.png" alt="Terminal output of initial forecast model results." class="responsive" width="80%">
                
                <p>Thankfully our ARIMA and ETS model do outperform our baselines. The <code>arima</code> model performs the best and <code>ets_aada</code> is second. The models with multiplicative seasonality don't perform as well and actually underperform some of our baselines.</p>
                
                <p>One cool feature of <code><a href="https://fable.tidyverts.org">fable</a></code> is that it makes it very easy to <a href="https://otexts.com/fpp3/combinations.html">combine or ensemble multiple forecast models.</a> Hyndman and Athanasopoulos describe this as such:</p>

                <blockquote class="highlighted">"The results have been virtually unanimous: combining multiple forecasts leads to increased forecast accuracy. In many cases one can make dramatic performance improvements by simply averaging the forecasts."</blockquote>

                <p>Based on our results, we'll try creating combined forecasts of the <code>arima</code> model with the <code>ets_aada</code> and <code>ets_aaa</code> models.</p>

                <pre><code class="language-r">
                    # Create combined models
                    fcst <- fit |> mutate(
                        arima_ets_aaa = (arima + ets_aaa) / 2,
                        arima_ets_aada = (arima + ets_aada) / 2,
                    ) |> forecast(h = months_to_forecast)
                    
                    # View model forecast accuracy against test set
                    accuracy(fcst, df) |>
                        arrange(RMSE)
                </code></pre>

                <img src="images/train-test-model-accuracy-add-combos.png" alt="Terminal output of forecast model results when adding combination models." class="responsive" width="80%">
                
                <p>After adding in our two combined models, the plain ARIMA model still performs the best, but the two combined models are second and third.</p>
                
                <p>We can plot the forecasts from each model to see how they compare:</p>

                <img src="images/train-test-top-forecast-models.png" alt="Line plot showing performance of top forecast models." class="responsive" width="80%">

                <p>Based on these results, I would pick either the <code>arima</code> or the <code>arima_ets_aada</code> model. While the <code>arima</code> model is more accurate compared to the test set, over the long term I would probably prefer to use an ensemble model.</p>
                
            <h2>Evaluating Models with Cross Validation</h2>
                <p>Another approach to time series model evaluation is <a href="https://otexts.com/fpp3/tscv.html">cross validation</a>. This is a little different from cross validation you may have used in other machine learning contexts. With time series data, we start with an initial base set of historical data, fit our models, calculate forecasts, and measure accuracy. We then repeat adding one (or more) data point to the base historical data, repeating the model fitting, forecasting, and testing for each step with. In each step we're forecasting one or more steps out. Prof. Hyndman uses this visualization in his textbook</p>
                <img src="images/fpp3-cross-validation.png" alt="" class="responsive" width="80%">
                
                <p>One neat benefit of this approach, is that we can see the average model accuracy across all of the CV sets at 1 to 12 steps out. As we'd expect, model accuracy gets worse the further out we try to forecast. We can see that the best performing model across the CV sets is the ETS(A, Ad, A) model, and the second best is the combination of ARIMA and ETS(A, Ad, A).</p>
                <img src="images/cross-validation-forecast-error.png" alt="" class="responsive" width="80%">
            
            <h2>Final Forecast</h2>
                <p></p>           
                         
            <h2>Conclusions</h2>
                <p></p>
            
            <a href="https://github.com/bbgatch/bbgatch.github.io/blob/main/projects/tsa/2024-09-20-forecasting-tsa-data-with-r/forecast.R">
            <h2>Full Code</h2>
            </a>
                <pre><code class="language-r">
                </code></pre>

        <!-- Footer -->
        <script language="javascript" type="text/javascript" src="https://www.bbgatch.com/footer.js"></script>

    </body>

</html>
